---
title: 'Time Series: A First Course with Bootstrap Starter'
output:
  html_document:
    df_print: paged
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/neide/Documents/GitHub/BEwebinar/Data')
```

# Lesson 12: Projection and Prediction

- We use *Hilbert Spaces* to think about time series prediction (i.e., forecasting).

## The Space ${\mathbb L}_2$

- For a given probability space, let ${\mathbb L}_2$ denote all random variables with finite second moment.
- Define an *inner product* on ${\mathbb L}_2$ as follows:
\[
  \langle X, Y \rangle = {\mathbb E} [ X Y ]
\]
 for $X,Y \in {\mathbb L}_2$.
- The *norm* is $\| X \| = \sqrt{ \langle X, X \rangle}$.
- ${\mathbb L}_2$ is a Hilbert Space.

## Remark 4.2.4. The Angle Between Two Random Variables

- We can think of the inner product as being related to the angle:
\[
  \cos (\theta) = \frac{ \langle X, Y \rangle}{ \| X \| \cdot \| Y \| },
\]
 where $\theta$ is interpreted as the angle between random variables $X$ and $Y$.
- Hence the inner product is zero if $\theta = \pi/2$, i.e., the random variables are *orthogonal*.

## Fact 4.4.2. Orthogonality Principle

- Suppose we want to project $Y \in {\mathbb L}_2$ onto a subspace $\mathcal{M} \subset {\mathbb L}_2$. Let $\widehat{Y} \in \mathcal{M}$ be the projection. The distance to the projection is $\| Y - \widehat{Y} \|$.
- The orthogonality principle states that the distance is minimized iff $Y - \widehat{Y}$ is orthogonal to all elements of $\mathcal{M}$.


## Paradigm 4.5.1. The Conditional Expectation

- Let $\{ X_t \}$ be a weakly stationary time series in ${\mathbb L}_2$. 
- Suppose for some $t > n$ we wish to predict $X_t$ from $X_1, \ldots, X_n$. The predictor is denoted $\widehat{X}_t$.
- We want the prediction error to have minimal mean square:
\[
  {\mathbb E} [ {( \widehat{X}_t - X_t)}^2]
\]
is the *Mean Squared Error* (MSE).  
- Theorem 4.5.2. The minimal MSE predictor is the conditional expectation:
\[
 \widehat{X}_t = {\mathbb E} [ X_t \vert X_1, \ldots, X_n].
\]

## Example 4.5.6. Order One Autoregression

- Let $\{ X_t \}$ be an AR(1), i.e., $X_t = \phi X_{t-1} + Z_t$ with $\{ Z_t \}$ i.i.d. $(0,\sigma^2)$. 
- Assume $Z_t$ is independent of $X_s$ for all $s < t$. 

### One-step ahead prediction

- Consider predicting one-step ahead: we want $\widehat{X}_{n+1}$, given $X_1, \ldots, X_n$.
- We calculate the conditional expectation:
\begin{align*}
  {\mathbf E} [ X_{n+1} \vert X_1, \ldots, X_n] & = 
     {\mathbf E} [ \phi X_n + Z_{n+1} \vert X_1, \ldots, X_n] \\
     & = \phi \,  {\mathbf E} [ X_{n} \vert X_1, \ldots, X_n]
      +  {\mathbf E} [ Z_{n+1} \vert X_1, \ldots, X_n] \\
      & = \phi \, X_n + 0.
\end{align*}
 This uses linearity of conditional expectation, and independence of $Z_{n+1}$ from $X_1, \ldots, X_n$.
- The prediction error is then
\[
  X_{n+1} - \widehat{X}_{n+1} = X_{n+1} - \phi \, X_n = Z_{n+1},
\]
 so that the MSE is ${\mathbb E} [ Z_{n+1}^2] = \sigma^2$.

```{r}
set.seed(123)
n <- 100
z <- rnorm(n)
x <- rep(0,n)
xhat <- rep(0,n)
phi <- .9
x0 <- 0
x[1] <- x0 + z[1]
for(t in 2:n) 
{
  x[t] <- phi*x[t-1] + z[t] 
  xhat[t] <- phi*x[t-1]
}
plot(ts(x),xlab="Time",ylab="")
lines(ts(xhat),col=2)
```

### Two-step ahead prediction

- Consider predicting two steps ahead: we want $\widehat{X}_{n+2}$, given $X_1, \ldots, X_n$.
- Note that by applying the AR(1) recursion twice we can write
\[
 X_{n+2} = \phi^2 X_n + \phi Z_{n+1} + Z_{n+2}.
 \]
- Hence the conditional expectation is
  \begin{align*}
  {\mathbf E} [ X_{n+2} \vert X_1, \ldots, X_n] & = 
     {\mathbf E} [ \phi^2 X_n + \phi Z_{n+1} + Z_{n+2} \vert X_1, \ldots, X_n] \\
     & = \phi^2 \,  {\mathbf E} [ X_{n} \vert X_1, \ldots, X_n]
      +  \phi \, {\mathbf E} [ Z_{n+1} \vert X_1, \ldots, X_n] 
      + {\mathbf E} [ Z_{n+2} \vert X_1, \ldots, X_n]        \\
      & = \phi^2 \, X_n + 0.
\end{align*}
- The prediction error is
\[
 X_{n+2} - \widehat{X}_{n+2} = \phi^2 X_n + \phi Z_{n+1} + Z_{n+2}
  - \phi^2 X_n = \phi Z_{n+1} + Z_{n+2}.
\]
 Hence the prediction MSE is $(1 + \phi^2) \sigma^2$.

```{r}
set.seed(123)
n <- 100
z <- rnorm(n)
x <- rep(0,n)
xhat <- rep(0,n)
phi <- .9
x0 <- 0
x[1] <- x0 + z[1]
x[2] <- phi*x[1] + z[2]
for(t in 3:n) 
{
  x[t] <- phi*x[t-1] + z[t] 
  xhat[t] <- phi^2*x[t-2]
}
plot(ts(x),xlab="Time",ylab="")
lines(ts(xhat),col=2)
```






