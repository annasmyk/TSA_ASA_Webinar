---
title: 'Time Series: A First Course with Bootstrap Starter'
output:
  html_document:
    df_print: paged
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = '/home/tucker/Documents/GitHub/BEwebinar/Data')
```

# Lesson 10-9: Information Criteria

- We discuss how to sift competing models.

## Remark 10.9.1. Underfitting and Overfitting

- *Underfitting* is specifying a wrong model, such that the model order is too low. Then the parameter estimates are not consistent, and there is bias.
- *Overfitting* is specifying a model with too high of a model order (or unneeded parameters). However, the true model is still nested within the specified model. In this case the parameter estimates are inefficient (their variance is higher than it would be if the model were correctly specified).
- Whenever we add model complexity, the Gaussian divergence decreases, and hence overfitting may arise.

## Definition 10.9.2.

- Suppose $\mathcal{F}$ is a linear time series model with parameter $\underline{\omega}$.
- If restricting some of the elements of $\underline{\omega}$ to zero yields another model $\mathcal{G}$, we say
that $\mathcal{G}$ is *nested* in $\mathcal{F}$, and $\mathcal{F}$ is *nests* in $\mathcal{G}$.

## Example 10.9.3. AR, MA, and ARMA Nesting

- Any AR($p+r$) nests an AR($p$), for $r > 0$: set the last $r$ components of $\underline{\omega}$ to zero, and you get the parameter vector of an AR($p$).
- Similarly for MA models.
- Similarly, an EXP($m+\ell$) nests an EXP($m$) model.
- An ARMA($p$,$q$) nests both an AR($p$) and an MA($q$) model.
- White noise is nested in all AR, MA, ARMA, and EXP models.

## Remark 10.9.5. Occam's Razor and the Principle of Parsimony

- If two models fit the data equally well, the simpler one is preferred.
- We can measure simplicity through the number of parameters.
- We can penalize the selection of models with a high number of parameters.
- This helps to guard against overfitting.

## Definition 10.9.6.

- The *Akaike Information Criterion* (AIC) for a linear model with $r$ parameters (not including the input variance $\sigma^2$) is given by the Gaussian divergence plus $2r$:
\[
  \mbox{AIC} (\underline{\omega}, \sigma^2) = 
   \mathcal{L} (\underline{\omega}, \sigma^2) + 2r.
\]
- Hence, minimizing AIC protects against overfitting.
- Using the profile likelihood, minimizing AIC is equivalent to minimizing
\[
   n \log \widehat{\sigma}^2 + 2r.
\]
- Note that $\widehat{\sigma}^2$ depends on $r$.

## Remark 10.9.7. Information Criteria

- Other penalties can be used. For example, the *Bayesian Information Criterion* (BIC) has penalty of $2r \log n$ instead of $2r$.
- AIC has a tendency towards a slight over-parameterization. This can be a good thing, as a safeguard against model mis-specification.

## Exercise 10.59. Information Criteria Model Comparison 

- We fit an AR(2) model and an MA(1) model to the Non-Defense Capitalization series.
- We must load up some functions.

```{r}
polymul <- function(a,b) 
{
	bb <- c(b,rep(0,length(a)-1))
	B <- toeplitz(bb)
	B[lower.tri(B)] <- 0
	aa <- rev(c(a,rep(0,length(b)-1)))
	prod <- B %*% matrix(aa,length(aa),1)
	return(rev(prod[,1]))
}

ARMAauto <- function(phi,theta,maxlag)
{
	p <- length(phi)
	q <- length(theta)
	gamMA <- polymul(c(1,theta),rev(c(1,theta)))
	gamMA <- gamMA[(q+1):(2*q+1)]
	if (p > 0) 
	{
		Amat <- matrix(0,nrow=(p+1),ncol=(2*p+1))
		for(i in 1:(p+1))
		{
			Amat[i,i:(i+p)] <- c(-1*rev(phi),1)
		}
		Amat <- cbind(Amat[,(p+1)],as.matrix(Amat[,(p+2):(2*p+1)]) +
			t(matrix(apply(t(matrix(Amat[,1:p],p+1,p)),2,rev),p,p+1)))
		Bmat <- matrix(0,nrow=(q+1),ncol=(p+q+1))
		for(i in 1:(q+1))
		{
			Bmat[i,i:(i+p)] <- c(-1*rev(phi),1)
		}
		Bmat <- t(matrix(apply(t(Bmat),2,rev),p+q+1,q+1))
		Bmat <- matrix(apply(Bmat,2,rev),q+1,p+q+1)
		Bmat <- Bmat[,1:(q+1)]
		Binv <- solve(Bmat)
		gamMix <- Binv %*% gamMA
		if (p <= q) { gamMix <- matrix(gamMix[1:(p+1),],p+1,1) 
			} else gamMix <- matrix(c(gamMix,rep(0,(p-q))),p+1,1)
		gamARMA <- solve(Amat) %*% gamMix 
	} else gamARMA <- gamMA[1]

	gamMA <- as.vector(gamMA)
	if (maxlag <= q) gamMA <- gamMA[1:(maxlag+1)] else gamMA <- c(gamMA,rep(0,(maxlag-q)))
	gamARMA <- as.vector(gamARMA)
	if (maxlag <= p) gamARMA <- gamARMA[1:(maxlag+1)] else {
	for(k in 1:(maxlag-p))
	{
		len <- length(gamARMA)
		acf <- gamMA[p+1+k]
		if (p > 0) acf <- acf + sum(phi*rev(gamARMA[(len-p+1):len]))
		gamARMA <- c(gamARMA,acf)
	} }
	return(gamARMA)
}

dl.alg <- function(gamma,data)
{
	# assumes that n > 1
	# assumes gamma includs lags 0 through n
	n <- length(data)
	kappa.vec <- gamma[2]/gamma[1]
	varphi.vec <- kappa.vec
	quad.new <- 0
	quads <- NULL
	ldet <- 0
	ldets <- NULL
	eps <- NULL

	for(k in 2:n)
	{
		schur.new <- gamma[1] - t(varphi.vec) %*% rev(gamma[2:k])
		eps.new <- data[k] - t(varphi.vec) %*% data[1:(k-1)]
		eps <- c(eps,eps.new)
		quad.new <- quad.new + eps.new^2/schur.new
		quads <- c(quads,quad.new)
		ldet <- ldet + log(schur.new)
		ldets <- c(ldets,ldet)
		kappa.new <- (gamma[k+1] - sum(varphi.vec*gamma[2:k]))/schur.new
		kappa.new <- kappa.new[1,1]
		if(abs(kappa.new) < 10^(-15)) kappa.new <- 0
		kappa.vec <- c(kappa.vec,kappa.new)	
		varphi.vec <- rev(varphi.vec) - kappa.new * varphi.vec
		varphi.vec <- rev(c(varphi.vec,kappa.new))
	}
	gauss.lik <- n*log(2*pi) + quads[n-1] + ldets[n-1]
	gauss.likprof <- n*(1 + log(2*pi)) + n*log(quads[n-1]/n) + ldets[n-1]

	return(list(gauss.lik,gauss.likprof,quads[n-1]/n,eps))
}

psi2phi <- function(psi)
{
	p <- length(psi)
	pacfs <- (exp(psi)-1)/(exp(psi)+1)
	if(p==1)
	{
		phi <- pacfs
	} else
	{
		phi <- as.vector(pacfs[1])
		for(j in 2:p)
		{
			A.mat <- diag(j-1) - pacfs[j]*diag(j-1)[,(j-1):1,drop=FALSE]
			phi <- A.mat %*% phi
			phi <- c(phi,pacfs[j])
		}
	}
	return(phi)
}

phi2psi <- function(phi)
{
	p <- length(phi)
	pacfs <- phi[p]
	if(p > 1)
	{
		phi <- as.vector(phi[-p])
		for(j in p:2)
		{
			A.mat <- diag(j-1) - pacfs[1]*diag(j-1)[,(j-1):1,drop=FALSE]
			phi <- solve(A.mat,phi)
			pacfs <- c(phi[j-1],pacfs)
			phi <- phi[-(j-1)]
		}
	}
	psi <- log(1+pacfs) - log(1-pacfs)
	return(psi)
}

likprof.arma <- function(psi,q,data)
{
	n <- length(data)
	r <- length(psi)
	p <- r-q
	phi <- NULL
	theta <- NULL
	if(p > 0) { phi <- psi2phi(psi[1:p]) }
	if(q > 0) { theta <- -1*psi2phi(psi[(p+1):(p+q)]) }
	gamma <- ARMAauto(phi,theta,n)
	lik <- dl.alg(gamma,data)[[2]]
	return(lik)
}
```

- We use the Gaussian likelihood to fit each model, and compare the fits via
AIC and BIC.

```{r}
ndc <- read.table("Nondefcap.dat")
ndc <- ts(ndc[,2],start=c(1992,3),frequency=12,names= "NewOrders")
ndc.diff <- diff(ndc)
n <- length(ndc.diff)
gamma.hat <- acf(ndc.diff,lag=n-1,type="covariance",plot=FALSE)$acf[,,1]
phi.init <- solve(toeplitz(gamma.hat[1:2]),gamma.hat[2:3])
```

- First we get Yule-Walker estimates (`r phi.init`) to initialize the MLE optimization.
- Then we find the MLEs.

```{r}
psi.init <- phi2psi(phi.init)
fit.ar2 <- optim(psi.init,likprof.arma,q=0,data=ndc.diff,method="BFGS")
```

- Second, we get the MLEs for the MA(1) model, intializing at zero.

```{r}
psi.init <- 0
fit.ma1 <- optim(psi.init,likprof.arma,q=1,data=ndc.diff,method="BFGS")
```

- We compute the AIC and BIC values next.

```{r}
lik.ar2 <- fit.ar2$value
lik.ma1 <- fit.ma1$value
aic.ar2 <- lik.ar2 + 2*2
bic.ar2 <- lik.ar2 + 2*2*log(n)
aic.ma1 <- lik.ma1 + 2*1
bic.ma1 <- lik.ma1 + 2*1*log(n)
```

- The AIC for the MA(1) and AR(2) models is `r aic.ma1` and `r aic.ar2` respectively.
- The BIC for the MA(1) and AR(2) models is `r bic.ma1` and `r bic.ar2` respectively.
- So according to AIC the AR(2) model is preferred, but by BIC the MA(1) model is preferred!
