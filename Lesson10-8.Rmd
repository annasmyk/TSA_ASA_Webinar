---
title: 'Time Series: A First Course with Bootstrap Starter'
output:
  html_document:
    df_print: paged
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/neide/Documents/GitHub/BEwebinar/Data')
```


# Lesson 21: Model Assessment

- Once we have fitted a model, we need to assess the fit.
- We should also compare to competing models.

## Remark 10.8.1. Computing Residuals

- If an ARMA model was fitted using the Gaussian divergence, we get the one-step-ahead forecast errors (in-sample) as output of the Durbin-Levinson algorithm. These are *time series residuals*.
- If our model is a good fit, the time series residuals should resemble white noise.

## Example 10.8.2. Residuals of MA(1) Model Fitted to Non-Defense Capitalization

- We continue Exercise 10.47 of previous lesson. First load all the needed functions.

```{r}
polymul <- function(a,b) 
{
	bb <- c(b,rep(0,length(a)-1))
	B <- toeplitz(bb)
	B[lower.tri(B)] <- 0
	aa <- rev(c(a,rep(0,length(b)-1)))
	prod <- B %*% matrix(aa,length(aa),1)
	return(rev(prod[,1]))
}

ARMAauto <- function(phi,theta,maxlag)
{
	p <- length(phi)
	q <- length(theta)
	gamMA <- polymul(c(1,theta),rev(c(1,theta)))
	gamMA <- gamMA[(q+1):(2*q+1)]
	if (p > 0) 
	{
		Amat <- matrix(0,nrow=(p+1),ncol=(2*p+1))
		for(i in 1:(p+1))
		{
			Amat[i,i:(i+p)] <- c(-1*rev(phi),1)
		}
		Amat <- cbind(Amat[,(p+1)],as.matrix(Amat[,(p+2):(2*p+1)]) +
			t(matrix(apply(t(matrix(Amat[,1:p],p+1,p)),2,rev),p,p+1)))
		Bmat <- matrix(0,nrow=(q+1),ncol=(p+q+1))
		for(i in 1:(q+1))
		{
			Bmat[i,i:(i+p)] <- c(-1*rev(phi),1)
		}
		Bmat <- t(matrix(apply(t(Bmat),2,rev),p+q+1,q+1))
		Bmat <- matrix(apply(Bmat,2,rev),q+1,p+q+1)
		Bmat <- Bmat[,1:(q+1)]
		Binv <- solve(Bmat)
		gamMix <- Binv %*% gamMA
		if (p <= q) { gamMix <- matrix(gamMix[1:(p+1),],p+1,1) 
			} else gamMix <- matrix(c(gamMix,rep(0,(p-q))),p+1,1)
		gamARMA <- solve(Amat) %*% gamMix 
	} else gamARMA <- gamMA[1]

	gamMA <- as.vector(gamMA)
	if (maxlag <= q) gamMA <- gamMA[1:(maxlag+1)] else gamMA <- c(gamMA,rep(0,(maxlag-q)))
	gamARMA <- as.vector(gamARMA)
	if (maxlag <= p) gamARMA <- gamARMA[1:(maxlag+1)] else {
	for(k in 1:(maxlag-p))
	{
		len <- length(gamARMA)
		acf <- gamMA[p+1+k]
		if (p > 0) acf <- acf + sum(phi*rev(gamARMA[(len-p+1):len]))
		gamARMA <- c(gamARMA,acf)
	} }
	return(gamARMA)
}

dl.alg <- function(gamma,data)
{
	# assumes that n > 1
	# assumes gamma includs lags 0 through n
	n <- length(data)
	kappa.vec <- gamma[2]/gamma[1]
	varphi.vec <- kappa.vec
	quad.new <- 0
	quads <- NULL
	ldet <- 0
	ldets <- NULL
	eps <- NULL

	for(k in 2:n)
	{
		schur.new <- gamma[1] - t(varphi.vec) %*% rev(gamma[2:k])
		eps.new <- data[k] - t(varphi.vec) %*% data[1:(k-1)]
		eps <- c(eps,eps.new)
		quad.new <- quad.new + eps.new^2/schur.new
		quads <- c(quads,quad.new)
		ldet <- ldet + log(schur.new)
		ldets <- c(ldets,ldet)
		kappa.new <- (gamma[k+1] - sum(varphi.vec*gamma[2:k]))/schur.new
		kappa.new <- kappa.new[1,1]
		if(abs(kappa.new) < 10^(-15)) kappa.new <- 0
		kappa.vec <- c(kappa.vec,kappa.new)	
		varphi.vec <- rev(varphi.vec) - kappa.new * varphi.vec
		varphi.vec <- rev(c(varphi.vec,kappa.new))
	}
	gauss.lik <- n*log(2*pi) + quads[n-1] + ldets[n-1]
	gauss.likprof <- n*(1 + log(2*pi)) + n*log(quads[n-1]/n) + ldets[n-1]

	return(list(gauss.lik,gauss.likprof,quads[n-1]/n,eps))
}

psi2phi <- function(psi)
{
	p <- length(psi)
	pacfs <- (exp(psi)-1)/(exp(psi)+1)
	if(p==1)
	{
		phi <- pacfs
	} else
	{
		phi <- as.vector(pacfs[1])
		for(j in 2:p)
		{
			A.mat <- diag(j-1) - pacfs[j]*diag(j-1)[,(j-1):1,drop=FALSE]
			phi <- A.mat %*% phi
			phi <- c(phi,pacfs[j])
		}
	}
	return(phi)
}

phi2psi <- function(phi)
{
	p <- length(phi)
	pacfs <- phi[p]
	if(p > 1)
	{
		phi <- as.vector(phi[-p])
		for(j in p:2)
		{
			A.mat <- diag(j-1) - pacfs[1]*diag(j-1)[,(j-1):1,drop=FALSE]
			phi <- solve(A.mat,phi)
			pacfs <- c(phi[j-1],pacfs)
			phi <- phi[-(j-1)]
		}
	}
	psi <- log(1+pacfs) - log(1-pacfs)
	return(psi)
}
```

- We use a profile likelihood for an ARMA process.

```{r}
likprof.arma <- function(psi,q,data)
{
	n <- length(data)
	r <- length(psi)
	p <- r-q
	phi <- NULL
	theta <- NULL
	if(p > 0) { phi <- psi2phi(psi[1:p]) }
	if(q > 0) { theta <- -1*psi2phi(psi[(p+1):(p+q)]) }
	gamma <- ARMAauto(phi,theta,n)
	lik <- dl.alg(gamma,data)[[2]]
	return(lik)
}
```

- We can fit the series as before.

```{r}
ndc <- read.table("Nondefcap.dat")
ndc <- ts(ndc[,2],start=c(1992,3),frequency=12,names= "NewOrders")
ndc.diff <- diff(ndc)
n <- length(ndc.diff)
psi.init <- 0
fit.ma1 <- optim(psi.init,likprof.arma,q=1,data=ndc.diff,method="BFGS")
psi.mle <- fit.ma1$par
theta.mle <- -1*psi2phi(psi.mle)
gamma <- ARMAauto(NULL,theta.mle,n)
sig2.mle <- dl.alg(gamma,ndc.diff)[[3]]
```

- So the parameter estimates are $\widehat{\theta_1} =$ `r theta.mle` and $\widehat{\sigma^2} =$ `r sig2.mle`.
- We can get the residuals as output as well.

```{r}
resid.mle <- dl.alg(gamma,ndc.diff)[[4]]
plot(ts(resid.mle,start=c(1992,4),frequency=12),
     xlab="Year",ylab="",lwd=1)
```

- Some structure in the residuals is evident around 2008 (the nadir of the Great Recession).
- We can examine the serial correlation structure using the sample acf (and pacf):

```{r}
acf(resid.mle)
```


## Remark 10.8.6. Portmanteau Statistics

- To assess whether the residuals resemble white noise, we can measure the autocovariances.
- We can combine into a single measure by taking a sum of squared autocorrelations.
- This is an aggregate measure, like a *portmanteau* (suitcase) of statistics.


## Definition 10.8.7.

- The *total variation* of a stationary time series with absolutely summable acvf is
\[
   \zeta = \sum_{k  \neq 0} { \gamma (k) }^2. 
\]
- Note that $\zeta = 0$ if and only if the process is white noise.

## Remark 10.8.9. Testing Total Variation

- So we can test whether a process is white noise by testing $\zeta = 0$, using the test statistic 
\[
 \widehat{\zeta} = \frac{1}{2n} \, \sum_{\ell = -[n/2]-n+1}^{[n/2]}
  { I (\lambda_{\ell} )  }^2  -  {\widehat{\gamma} (0)}^2.
\]
- It can be shown that
\[
 \sqrt{n} \, \widehat{\zeta} \Rightarrow \mathcal{N} (0, 2 \sigma^8)
\]
 under $H_0: \zeta = 0$, where $\sigma^2$ is the variance of the process.
- A normalized test statistic that is asymptotically standard normal is given by
\[
   \sqrt{n/2} \, \frac{ \widehat{\zeta}}{ { \widehat{\gamma} (0)}^2 }.
\]
- We perform an upper one-sided test; large values indicate that serial correlation is present.
- We can apply this to the time series residuals.


## Example 10.8.10.  Total Variation White Noise Testing of Non-Defense Capitalization

- We apply the total variation test statistic to the time series residuals from the fit of an MA(1) to Non-Defense Capitalization.

```{r}
n <- length(resid.mle)
gamma.hat <- acf(resid.mle,lag.max=n,plot=FALSE,type="covariance")$acf
lambda <- seq(-n/2+1,n/2)*2*pi/n
pgram <- cos(0*lambda)*gamma.hat[1]
for(h in 1:(n-1))
{
	pgram <- pgram + 2*cos(h*lambda)*gamma.hat[h+1]
}
pgram <- ts(pgram,start=0,frequency=n)
tot.var <- .5*mean(pgram^2) - gamma.hat[1]^2
tstat <- sqrt(n)*tot.var/sqrt(2*gamma.hat[1]^4)
pval <- 1-pnorm(tstat)
```

- The test statistic is `r tot.var`, and the studentized statistic is `r tstat`, with asymptotic p-value `r pval`.
- So there is no evidence for rejecting $H_0$, and we conclude that no serial correlation remains.

## Remark 10.9.1. Underfitting and Overfitting

- Underfitting is specifying a wrong model, such that the model order is too low. Then the parameter estimates are not consistent, and there is bias.
- Overfitting is specifying a model with too high of a model order. However, the true model is still nested within the specified model. In this case the parameter estimates are inefficient (their variance is higher than it would be if the model were correctly specified).
- Whenever we add model complexity, the Gaussian divergence decreases, and hence overfitting may arise.

## Definition 10.9.6.

- The *Akaike Information Criterion* (AIC) for a linear model with $r$ parameters (not including the input variance $\sigma^2$) is given by the Gaussian divergence plus $2r$:
\[
  \mbox{AIC} (\underline{\omega}, \sigma^2) = 
   \mathcal{L} (\underline{\omega}, \sigma^2) + 2r.
\]
- This adds a penalty for models with many parameters.
- Hence, minimizing AIC protects against overfitting.
- Using the profile likelihood, minimizing AIC is equivalent to minimizing
\[
   n \log \widehat{\sigma}^2 + 2r.
\]
- Note that $\widehat{\sigma}^2$ depends on $r$.
- Other penalties can be used. For example, the *Bayesian Information Criterion* (BIC) has penalty of $2r \log n$ instead of $2r$.
- AIC has a tendency towards a slight over-parameterization. This can be a good thing, as a safeguard against model mis-specification.

## Exercise 10.59. Information Criteria Model Comparison 

- We fit an AR(2) model to the Non-Defense Capitalization series.

```{r}
n <- length(ndc.diff)
gamma.hat <- acf(ndc.diff,lag=n-1,type="covariance",plot=FALSE)$acf[,,1]
phi.init <- solve(toeplitz(gamma.hat[1:2]),gamma.hat[2:3])
```

- First get Yule-Walker estimates (`r phi.init`) to initialize the MLE optimization.

```{r}
psi.init <- phi2psi(phi.init)
fit.ar2 <- optim(psi.init,likprof.arma,q=0,data=ndc.diff,method="BFGS")
psi.mle <- fit.ar2$par
phi.mle <- psi2phi(psi.mle)
gamma <- ARMAauto(phi.mle,NULL,n)
sig2.mle <- dl.alg(gamma,ndc.diff)[[3]]
```

- We compute the AIC and BIC values next.

```{r}
lik.ar2 <- fit.ar2$value
lik.ma1 <- fit.ma1$value
aic.ar2 <- lik.ar2 + 2*2
bic.ar2 <- lik.ar2 + 2*2*log(n)
aic.ma1 <- lik.ma1 + 2*1
bic.ma1 <- lik.ma1 + 2*1*log(n)
```

- The AIC for the MA(1) and AR(2) models is `r aic.ma1` and `r aic.ar2` respectively.
- The BIC for the MA(1) and AR(2) models is `r bic.ma1` and `r bic.ar2` respectively.
- So according to AIC the AR(2) model is preferred, but by BIC the MA(1) model is preferred!
