---
title: 'Time Series: A First Course with Bootstrap Starter'
output:
  html_document:
    df_print: paged
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/neide/Documents/GitHub/BEwebinar/Data')
```

# Lesson 10-5: Relative Entropy Minimization

- We can fit AR(p) models using the sample Yule-Walker equations (Paradigm 10.4.2), or using OLS.
- We can fit MA(q) models using a *spectral factorization*, which is akin to Yule-Walker in that it is a method-of-moments approach.
- Here we focus on Maximum Likelihood Estimation (MLE).

## Remark 10.5.1. Connection to Relative Entropy

## Remark 10.5.3. The Whittle Estimator

## Paradigm 10.5.9. The Gaussian Log Likelihood

- We consider the log likelihood of a length $n$ sample from a stationary Gaussian time series. Multiplying by $-2$, we obtain the *Gaussian divergence*:
\[
  \mathcal{L} (\underline{\omega}, \sigma^2) =
   n\log (2 \pi) + \log \det \Gamma_n + {( \underline{X} - \underline{\mu})}^{\prime} \,
    \Gamma_n^{-1} \, {( \underline{X} - \underline{\mu})}.
\]
- Here $\underline{\mu}$ is the mean vector, and equals the mean $\mu$ times a vector of ones.
- Also $\underline{\omega}$ is the parameter vector (all the parameters except the input variance $\sigma^2$).
- Note that for AR, MA, and ARMA models, there is a parameter vector $\underline{\omega}$, which together with $\sigma^2$ determines $\gamma (h)$, and thereby $\Gamma_n$.
- To fit a model, we seek to minimize the Gaussian divergence over $\underline{\omega}$ and $\sigma^2$. The resulting minimizers are the MLEs $\widehat{\underline{\omega}}$ and $\widehat{\sigma^2}$.

## Remark 10.5.10. Profile Gaussian Log Likelihood

- For many models, we can factor out $\sigma^2$ from each $\gamma (h)$.
- In such a case, write $\upsilon (h) = \gamma (h) / \sigma^2$, and construct the Toeplitz $\Upsilon_n$ from these $\upsilon (h)$ values.
- Then the divergence becomes
\[
  \mathcal{L} (\underline{\omega}, \sigma^2) =
   n\log (2 \pi) + n \log \sigma^2 +
   \log \det \Upsilon_n + \sigma^{-2} {( \underline{X} - \underline{\mu})}^{\prime} \,
    \Upsilon_n^{-1} \, {( \underline{X} - \underline{\mu})}.
\]
- Use calculus: differentiate with respect to $\sigma^2$, set equal to zero, and solve.
- Get a minimizer $\widehat{\sigma^2}$ that is a function of the still unknown $\underline{\omega}$. 
- Plugging this formula back in to the divergence is called *concentration*, and we get a *profile likelihood* $\mathcal{L} (\underline{\omega},\widehat{\sigma^2})$.
- The result is
\begin{align*}
  \widehat{\sigma^2} & = n^{-1} {( \underline{X} - \underline{\mu})}^{\prime} \,
    \Upsilon_n^{-1} \, {( \underline{X} - \underline{\mu})}  \\
  \mathcal{L} (\underline{\omega},\widehat{\sigma^2}) & = 
    n \log (2\pi) + (n+1) \log \widehat{\sigma^2} + \log \det \Upsilon_n.
\end{align*}
- So we can minimize the profile likelihood as a function of $\underline{\omega}$, obtaining $\widehat{\underline{\omega}}$, and then finally get the input variance MLE via plugging into the first formula.
- For large $n$, it can be shown that $n^{-1} \log \det \Upsilon_n \approx 0$, which implies we can dump the last term in the profile likelihood. Then approximately, we can just minimize $\widehat{\sigma^2}$, which is interpretable as a measure of one-step ahead forecast mean squared error.

## Exercise 10.34. Computing the Whittle Likelihood for MA Processes

## Exercise 10.42. Profile Whittle Likelihood

## Exercise 10.46. Fitting via the Whittle Likelihood

