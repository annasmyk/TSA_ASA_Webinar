title: 'Time Series: A First Course with Bootstrap Starter'
output:
  html_document:
    df_print: paged
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = '/home/tucker/Documents/GitHub/BEwebinar/Data')
```

# Lesson 9-4: Serial Correlation

We want to estimate the ACVF: they are used in prediction, and are helpful in understanding serial dependence in the time series.

## Remark 9.4.1. ACVF Estimator for Known Mean

- Suppose that the mean $\mu$ of the stationary time series $\{ X_t \}$ was known, and we want to estimate $\gamma (h)$.
- This is the expectation of $Y_t$, where
\[
 Y_t = (X_{t+h} - \mu)(X_t - \mu).
\]
- We can compute $Y_1,\ldots, Y_{n-h}$ for any $0 \leq h < n$, because $\mu$ is known.
- The sample mean of these would be
\[
   \frac{1}{n-h} \sum_{t=1}^{n-h} Y_t =
   \frac{1}{n-h} \sum_{t=1}^{n-h} (X_{t+h} - \mu)(X_t - \mu).
\]
 This has expectation $\gamma (h)$.
- The long-run variance of this sample mean is based on the ACVF of $\{ Y_t \}$, and has a complicated expression known as *Bartlett's Formula*.
- This suggests an estimator
\[
  \overline{\gamma} (h) = \frac{1}{n-h} \sum_{t=1}^{n-h} (X_{t+h} - \mu)(X_t - \mu),
\]
 which is unbiased for $\gamma (h)$.

## Proposition 9.4.3. Bartlett's Formula

Suppose $\{ X_t \}$ is a causal linear time series with i.i.d. inputs $\{ Z_t \}$, which have variance $\sigma^2$ and kurtosis $\eta$. Then the long-run variance of $\overline{\gamma}(h)$ is
\[
  \tau^2_{\infty} = \sum_{k = -\infty}^{\infty} \left(  \gamma (k+h) \gamma (k-h) + { \gamma (k)}^2 \right)
    + { \gamma (h)}^2 (\eta - 3).
\]

## Remark 9.4.6. ACVF Estimator for Unknown Mean

- Since $\mu$ is usually unknown, we can replace it by the sample mean $\overline{X}$.
- The resulting estimator is asymptotically normal when the process $\{ X_t \}$ is $m$-dependent (i.e., random variables of at least lag $m$ between them are independent).

