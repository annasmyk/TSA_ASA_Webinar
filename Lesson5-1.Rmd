---
title: 'Time Series: A First Course with Bootstrap Starter'
output:
  html_document:
    df_print: paged
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = 'C:/Users/neide/OneDrive/Documents/GitHub/BEwebinar/Data')
```

# Lesson 5-1: ARMA Processes

- ARMA processes generalize the AR and MA processes, and are central to classical time series analysis. They are very useful for modeling and forecasting stationary time series data.

## Definition 5.1.1.

- $\{ X_t \}$ is an ARMA($p$,$q$) process if it is stationary and satisfies
\[
  X_t - \sum_{j=1}^p \phi_j \, X_{t-j} = Z_t + \sum_{j=1}^q \theta_j \, Z_{t-j},
\]
 where $Z_t \sim \mbox{WN} (0,\sigma^2)$. The $\{ Z_t \}$ process is called the *inputs*.
- This is a recursive definition. It requires $p$ initial conditions to start the process.
- An ARMA is like an AR process with MA inputs.
- Special cases: $p=0$ gives an MA($q$), and $q=0$ gives an AR($p$).


## Paradigm 5.1.3. ARMA as a Linear Filter

- We can compactly write the ARMA equation in terms of the backward shift operator $B$. Define the polynomials
\[
 \phi (z) =  1 - \sum_{j=1}^p \phi_j z^j \qquad \theta (z) = 1 + \sum_{j=1}^q \theta_j z^j.
 \]
 Then the ARMA process satisfies
\[
 \phi (B) X_t = \theta (B) Z_t.
\]

## Example 5.1.4. MA(q) Autocovariance

- Take $p=0$ but $q > 0$, and determine the autocovariance function.
- Suppose $h \geq 0$:
\[
 {\mathbf E} [ X_t X_{t+h}] = \sum_{j=0}^q \sum_{k=0}^q \theta_j \theta_k {\mathbb E} [ Z_{t-j} Z_{t+h-k}]
  = \sum_{j=0}^{q-h} \theta_j \theta_{j+h} \, \sigma^2,
\]
where the sum is interpreted as zero if $q< h$. 
- The second equality follows from white noise: ${\mathbb E} [ Z_{t-j} Z_{t+h-k}] = 0$ unless
$t-j = t+h -k$, or $j = k-h$. So the double sum collapses to a single sum, setting $k=j+h$.
- But if $h > q$, then it's impossible for $k = j +h$, because $j +h \geq h > q \geq k$.
- This formula is a convolution of the sequence $\{ \theta_j \}$ with its reverse!

## Exercise 5.14. Product of Polynomials

- We can quickly compute the autocovariance function for an MA process by convolution
of the moving average polynomial with its reverse. 
- The convolution is also obtained by reading off the product of polynomials.
- We apply this idea to numerically compute the autocovariance for an MA(3) with
$\theta(z) = 1 + .4z + .2z^2 - .3z^3$, and $\sigma = 1$.
- First we write a routine to multiply polynomials.

```{r}
polymul <- function(a,b) 
{
	bb <- c(b,rep(0,length(a)-1))
	B <- toeplitz(bb)
	B[lower.tri(B)] <- 0
	aa <- rev(c(a,rep(0,length(b)-1)))
	prod <- B %*% matrix(aa,length(aa),1)
	return(rev(prod[,1]))
}
```

- Then we define the polynomial, and take its product with itself *reversed*.
- This will yield the autocovariance at lags $-3, -2, -1, 0, 1, 2, 3$.

```{r}
theta <- c(1,.4,.2,-.3)
gamma <- polymul(theta,rev(theta))
print(gamma)
```

## Example 5.1.5. MA($\infty$) Process.

- Letting $q = \infty$ in Example 5.1.4, we obtain the important MA($\infty$) process:
$X_t = \sum_{j \geq 0} \theta_j Z_{t-j}$.
- Assumes the coefficients satisfy $\sum_{j \geq 0} \theta_j^2 < \infty$.
- The autocovariance function is
\[
 \gamma (h) = \sigma^2 \, \sum_{j =0}^{\infty} \theta_j \theta_{j+h}.
\]

